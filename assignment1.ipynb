{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfqVWD93aVig"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12871,
     "status": "ok",
     "timestamp": 1756651473753,
     "user": {
      "displayName": "김성은",
      "userId": "08880536276811031210"
     },
     "user_tz": -540
    },
    "id": "Oxbhe1cpciwT",
    "outputId": "7ea25ddb-8f85-42a0-a4ce-e5ccff0e4872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyDlaVwCdZGV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split,Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os,glob\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2crx6nrlcXdg"
   },
   "source": [
    "# Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55_dmcT8bQW0"
   },
   "outputs": [],
   "source": [
    "img_dir = \"/content/drive/MyDrive/DIV2K_519sampled\"\n",
    "\n",
    "crop_size = 64\n",
    "upscale_factor = 2\n",
    "\n",
    "hrtransform = transforms.Compose([\n",
    "    transforms.CenterCrop(crop_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "lrtransform = transforms.Compose([\n",
    "    transforms.CenterCrop(crop_size),\n",
    "    transforms.Resize(crop_size//upscale_factor,interpolation = Image.BICUBIC),\n",
    "    transforms.Resize(crop_size,interpolation = Image.BICUBIC),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_size = 363\n",
    "valid_size = 104\n",
    "test_size = 52\n",
    "\n",
    "class mydataset(Dataset):\n",
    "  def __init__(self,img_dir,start,end):\n",
    "    super().__init__()\n",
    "    self.image = glob.glob(os.path.join(img_dir,\"*.png\"))\n",
    "    self.image = sorted(self.image)\n",
    "    self.image = self.image[start:end]\n",
    "    self.img_dir = img_dir\n",
    "    self.hrtransforms = hrtransform\n",
    "    self.lrtransforms = lrtransform\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    image = Image.open(self.image[index])\n",
    "    target = image.copy()\n",
    "    lr = self.lrtransforms(target)\n",
    "    hr = self.hrtransforms(image)\n",
    "    return lr,hr\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image)\n",
    "\n",
    "train_dataset = mydataset(img_dir,0,train_size)\n",
    "valid_dataset = mydataset(img_dir,train_size,train_size+valid_size)\n",
    "test_dataset = mydataset(img_dir,train_size+valid_size,train_size+valid_size+test_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size =32,shuffle = True)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size =32,shuffle = False)\n",
    "test_loader = DataLoader(test_dataset,batch_size =32,shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU2pwnFwekLb"
   },
   "source": [
    "# Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3ibIvIXbRFD"
   },
   "outputs": [],
   "source": [
    "class srcnn(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(3,64,9,1,4),\n",
    "        nn.ReLU(inplace = True),\n",
    "    )\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(64,32,1,1,0),\n",
    "        nn.ReLU(inplace = True),\n",
    "    )\n",
    "    self.layer3 = nn.Sequential(\n",
    "        nn.Conv2d(32,3,5,1,2),\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy5gI0Gieu33"
   },
   "source": [
    "# Model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2365646,
     "status": "ok",
     "timestamp": 1756653875648,
     "user": {
      "displayName": "김성은",
      "userId": "08880536276811031210"
     },
     "user_tz": -540
    },
    "id": "arUzQg_4bR4t",
    "outputId": "4b7746aa-aa37-4553-e714-45dcc461fa78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0, Loss: 0.0729\n",
      "Epoch [1/50], Validation Loss: 0.0329\n",
      "Epoch 1, Loss: 0.0263\n",
      "Epoch [2/50], Validation Loss: 0.0197\n",
      "Epoch 2, Loss: 0.0181\n",
      "Epoch [3/50], Validation Loss: 0.0156\n",
      "Epoch 3, Loss: 0.0131\n",
      "Epoch [4/50], Validation Loss: 0.0109\n",
      "Epoch 4, Loss: 0.0098\n",
      "Epoch [5/50], Validation Loss: 0.0087\n",
      "Epoch 5, Loss: 0.0079\n",
      "Epoch [6/50], Validation Loss: 0.0076\n",
      "Epoch 6, Loss: 0.0072\n",
      "Epoch [7/50], Validation Loss: 0.0068\n",
      "Epoch 7, Loss: 0.0063\n",
      "Epoch [8/50], Validation Loss: 0.0060\n",
      "Epoch 8, Loss: 0.0058\n",
      "Epoch [9/50], Validation Loss: 0.0054\n",
      "Epoch 9, Loss: 0.0053\n",
      "Epoch [10/50], Validation Loss: 0.0057\n",
      "Epoch 10, Loss: 0.0051\n",
      "Epoch [11/50], Validation Loss: 0.0044\n",
      "Epoch 11, Loss: 0.0044\n",
      "Epoch [12/50], Validation Loss: 0.0042\n",
      "Epoch 12, Loss: 0.0042\n",
      "Epoch [13/50], Validation Loss: 0.0040\n",
      "Epoch 13, Loss: 0.0040\n",
      "Epoch [14/50], Validation Loss: 0.0039\n",
      "Epoch 14, Loss: 0.0039\n",
      "Epoch [15/50], Validation Loss: 0.0038\n",
      "Epoch 15, Loss: 0.0038\n",
      "Epoch [16/50], Validation Loss: 0.0038\n",
      "Epoch 16, Loss: 0.0037\n",
      "Epoch [17/50], Validation Loss: 0.0037\n",
      "Epoch 17, Loss: 0.0037\n",
      "Epoch [18/50], Validation Loss: 0.0035\n",
      "Epoch 18, Loss: 0.0036\n",
      "Epoch [19/50], Validation Loss: 0.0037\n",
      "Epoch 19, Loss: 0.0037\n",
      "Epoch [20/50], Validation Loss: 0.0035\n",
      "Epoch 20, Loss: 0.0034\n",
      "Epoch [21/50], Validation Loss: 0.0033\n",
      "Epoch 21, Loss: 0.0033\n",
      "Epoch [22/50], Validation Loss: 0.0032\n",
      "Epoch 22, Loss: 0.0032\n",
      "Epoch [23/50], Validation Loss: 0.0032\n",
      "Epoch 23, Loss: 0.0032\n",
      "Epoch [24/50], Validation Loss: 0.0031\n",
      "Epoch 24, Loss: 0.0031\n",
      "Epoch [25/50], Validation Loss: 0.0031\n",
      "Epoch 25, Loss: 0.0034\n",
      "Epoch [26/50], Validation Loss: 0.0035\n",
      "Epoch 26, Loss: 0.0032\n",
      "Epoch [27/50], Validation Loss: 0.0030\n",
      "Epoch 27, Loss: 0.0031\n",
      "Epoch [28/50], Validation Loss: 0.0031\n",
      "Epoch 28, Loss: 0.0031\n",
      "Epoch [29/50], Validation Loss: 0.0031\n",
      "Epoch 29, Loss: 0.0030\n",
      "Epoch [30/50], Validation Loss: 0.0030\n",
      "Epoch 30, Loss: 0.0029\n",
      "Epoch [31/50], Validation Loss: 0.0029\n",
      "Epoch 31, Loss: 0.0028\n",
      "Epoch [32/50], Validation Loss: 0.0028\n",
      "Epoch 32, Loss: 0.0029\n",
      "Epoch [33/50], Validation Loss: 0.0028\n",
      "Epoch 33, Loss: 0.0028\n",
      "Epoch [34/50], Validation Loss: 0.0028\n",
      "Epoch 34, Loss: 0.0027\n",
      "Epoch [35/50], Validation Loss: 0.0028\n",
      "Epoch 35, Loss: 0.0027\n",
      "Epoch [36/50], Validation Loss: 0.0028\n",
      "Epoch 36, Loss: 0.0029\n",
      "Epoch [37/50], Validation Loss: 0.0028\n",
      "Epoch 37, Loss: 0.0028\n",
      "Epoch [38/50], Validation Loss: 0.0027\n",
      "Epoch 38, Loss: 0.0027\n",
      "Epoch [39/50], Validation Loss: 0.0027\n",
      "Epoch 39, Loss: 0.0030\n",
      "Epoch [40/50], Validation Loss: 0.0029\n",
      "Epoch 40, Loss: 0.0031\n",
      "Epoch [41/50], Validation Loss: 0.0034\n",
      "Epoch 41, Loss: 0.0028\n",
      "Epoch [42/50], Validation Loss: 0.0029\n",
      "Epoch 42, Loss: 0.0027\n",
      "Epoch [43/50], Validation Loss: 0.0026\n",
      "Epoch 43, Loss: 0.0027\n",
      "Epoch [44/50], Validation Loss: 0.0027\n",
      "Epoch 44, Loss: 0.0026\n",
      "Epoch [45/50], Validation Loss: 0.0026\n",
      "Epoch 45, Loss: 0.0026\n",
      "Epoch [46/50], Validation Loss: 0.0026\n",
      "Epoch 46, Loss: 0.0026\n",
      "Epoch [47/50], Validation Loss: 0.0026\n",
      "Epoch 47, Loss: 0.0026\n",
      "Epoch [48/50], Validation Loss: 0.0026\n",
      "Epoch 48, Loss: 0.0025\n",
      "Epoch [49/50], Validation Loss: 0.0025\n",
      "Epoch 49, Loss: 0.0025\n",
      "Epoch [50/50], Validation Loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = srcnn().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  total_loss = 0.0\n",
    "  for image,target in train_loader:\n",
    "    image = image.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    output = model(image)\n",
    "    loss = criterion(output,target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for image, target in valid_loader:\n",
    "      image = image.to(device)\n",
    "      target = target.to(device)\n",
    "\n",
    "      output = model(image)\n",
    "      loss = criterion(output,target)\n",
    "\n",
    "      val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjs242gFe5sx"
   },
   "source": [
    "# Model 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1756654269992,
     "user": {
      "displayName": "김성은",
      "userId": "08880536276811031210"
     },
     "user_tz": -540
    },
    "id": "_jtqSv2wbR_0",
    "outputId": "c7e327cc-39fd-4952-e772-ab63fd8296b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srcnn(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(),\"/content/drive/MyDrive/srcnn_checkpoint.pth\")\n",
    "new_model = srcnn().to(device)\n",
    "new_model.load_state_dict(torch.load(\"srcnn_checkpoint.pth\"))\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZppMf0VbSFx"
   },
   "outputs": [],
   "source": [
    "save_dir = \"/content/drive/MyDrive/SR_outputs\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "  for idx,batch in enumerate(test_loader):\n",
    "    image = batch[0].to(device)\n",
    "    output = model(image)\n",
    "    for i in range(output.size(0)):\n",
    "      output_img = output[i].cpu().clamp(0, 1)\n",
    "      save_path = os.path.join(save_dir, f\"SR_output_{idx * test_loader.batch_size + i + 1}.png\")\n",
    "      F.to_pil_image(output_img).save(save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO0iez/U4KrE7KSLryJt/p6",
   "gpuType": "T4",
   "mount_file_id": "1aayhAQj58Ne-HdjydwSiMk3q0xosb6eB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
